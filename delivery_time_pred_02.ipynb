{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7clLjrQ0rVx",
        "outputId": "7296506e-1ed1-4f34-c2eb-0a2de38e3eb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKFX03CH0qOp",
        "outputId": "c7d9a70b-7c9d-40ab-f7db-db8d13d731ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15-3100086836.py:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_selected['Order_Time'] = pd.to_datetime(df_selected['Order_Time'])\n",
            "/tmp/ipython-input-15-3100086836.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_selected['Pickup_Time'] = pd.to_datetime(df_selected['Pickup_Time'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
            "Best parameters: {'subsample': 1.0, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 15709.2549 - val_loss: 4255.7441 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 3115.7847 - val_loss: 2030.1581 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 2207.1680 - val_loss: 2027.9120 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 2162.5334 - val_loss: 2004.8718 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2175.6304 - val_loss: 2030.3141 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 2116.8066 - val_loss: 2016.5388 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - loss: 2180.9614 - val_loss: 1927.5863 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 2048.5249 - val_loss: 1876.4137 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 2035.0018 - val_loss: 1829.1871 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 1961.2769 - val_loss: 1823.6322 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 1995.5160 - val_loss: 1823.4591 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 14ms/step - loss: 1959.6742 - val_loss: 1786.8396 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 1923.4716 - val_loss: 1774.4420 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 1909.2458 - val_loss: 1783.2188 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 1900.0045 - val_loss: 1758.1027 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 1903.3160 - val_loss: 1789.7577 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - loss: 1906.7122 - val_loss: 1793.9319 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 1935.5918 - val_loss: 1768.2922 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 1908.2023 - val_loss: 1765.7953 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 1918.7892 - val_loss: 1771.7944 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - loss: 1924.2782 - val_loss: 1750.9478 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1888.1975 - val_loss: 1737.8461 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 1893.4293 - val_loss: 1748.5787 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - loss: 1881.7365 - val_loss: 1740.8048 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 1885.9032 - val_loss: 1738.0082 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 1850.4318 - val_loss: 1743.8743 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 1876.8112 - val_loss: 1727.2947 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 1841.1180 - val_loss: 1741.3269 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 1905.5345 - val_loss: 1729.9740 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 1882.8411 - val_loss: 1730.3015 - learning_rate: 5.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 1868.9397 - val_loss: 1753.0811 - learning_rate: 5.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1866.8986 - val_loss: 1776.7969 - learning_rate: 5.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1835.8374 - val_loss: 1739.8013 - learning_rate: 2.5000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 1828.8474 - val_loss: 1743.3495 - learning_rate: 2.5000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 1839.9836 - val_loss: 1767.1581 - learning_rate: 2.5000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1844.5272 - val_loss: 1729.5001 - learning_rate: 2.5000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 1835.2070 - val_loss: 1752.8717 - learning_rate: 2.5000e-04\n",
            "XGBoost RMSE: 40.91300704553275\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "DNN RMSE: 41.5896513171208\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "file_path = \"amazon_delivery.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "selected_columns = [\n",
        "    'Store_Latitude', 'Store_Longitude', 'Drop_Latitude', 'Drop_Longitude',\n",
        "    'Order_Date', 'Order_Time', 'Pickup_Time', 'Weather', 'Traffic',\n",
        "    'Vehicle', 'Area', 'Delivery_Time'\n",
        "]\n",
        "df_selected = df[selected_columns]\n",
        "\n",
        "df_selected = df_selected.dropna()\n",
        "\n",
        "df_selected['Order_Date'] = pd.to_datetime(df_selected['Order_Date'])\n",
        "df_selected['Day_of_Week'] = df_selected['Order_Date'].dt.dayofweek\n",
        "df_selected['Order_Time'] = pd.to_datetime(df_selected['Order_Time'])\n",
        "df_selected['Pickup_Time'] = pd.to_datetime(df_selected['Pickup_Time'])\n",
        "df_selected['Time_To_Pickup'] = (df_selected['Pickup_Time'] - df_selected['Order_Time']).dt.total_seconds() / 60.0\n",
        "\n",
        "df_selected = df_selected.drop(columns=['Order_Date', 'Order_Time', 'Pickup_Time'])\n",
        "\n",
        "df_selected = pd.get_dummies(df_selected, columns=['Weather', 'Traffic', 'Vehicle', 'Area'])\n",
        "\n",
        "df_selected['Distance'] = np.sqrt(\n",
        "    (df_selected['Drop_Latitude'] - df_selected['Store_Latitude'])**2 +\n",
        "    (df_selected['Drop_Longitude'] - df_selected['Store_Longitude'])**2\n",
        ")\n",
        "\n",
        "X = df_selected.drop(columns=['Delivery_Time'])\n",
        "y = df_selected['Delivery_Time']\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'reg_alpha': [0, 1],\n",
        "    'reg_lambda': [0.1, 1]\n",
        "}\n",
        "\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_grid,\n",
        "                                   n_iter=100, scoring='neg_mean_squared_error',\n",
        "                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "xg_reg_best = xgb.XGBRegressor(**best_params)\n",
        "xg_reg_best.fit(X_train, y_train)\n",
        "\n",
        "def create_dnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "dnn_model = create_dnn_model()\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "history = dnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,\n",
        "                        callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "y_pred_xgboost = xg_reg_best.predict(X_test)\n",
        "rmse_xgboost = np.sqrt(mean_squared_error(y_test, y_pred_xgboost))\n",
        "print(f\"XGBoost RMSE: {rmse_xgboost}\")\n",
        "\n",
        "y_pred_dnn = dnn_model.predict(X_test)\n",
        "rmse_dnn = np.sqrt(mean_squared_error(y_test, y_pred_dnn))\n",
        "print(f\"DNN RMSE: {rmse_dnn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save XGBoost model\n",
        "xg_reg_best.save_model(\"xgboost_model.json\")\n",
        "\n",
        "# Save DNN model\n",
        "dnn_model.save(\"dnn_model.h5\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"xgboost_model.json\")\n",
        "files.download(\"dnn_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "iLPuYjYSUiC7",
        "outputId": "5d2278fa-606f-47b2-86b9-c42455f4e6b2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cad405a7-f1d2-4353-8a66-cc7adca70515\", \"xgboost_model.json\", 232966)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85f901dc-5c44-4ae0-9211-7ad466a1221d\", \"dnn_model.h5\", 4031536)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Predict using both trained models\n",
        "y_pred_xgb = xg_reg_best.predict(X_test)\n",
        "y_pred_dnn = dnn_model.predict(X_test).flatten()\n",
        "\n",
        "# Search for best weight to combine them\n",
        "best_rmse = float(\"inf\")\n",
        "best_weight = 0.0\n",
        "\n",
        "weights = np.linspace(0, 1, 101)  # Try weights from 0.00 to 1.00 (step of 0.01)\n",
        "\n",
        "for w in weights:\n",
        "    y_pred_combined = w * y_pred_xgb + (1 - w) * y_pred_dnn\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_combined))\n",
        "\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse = rmse\n",
        "        best_weight = w\n",
        "\n",
        "# Final ensemble prediction using the best weight\n",
        "y_pred_ensemble = best_weight * y_pred_xgb + (1 - best_weight) * y_pred_dnn\n",
        "\n",
        "print(f\"✅ Best Ensemble RMSE: {best_rmse:.4f}\")\n",
        "print(f\"📊 Best Weight → XGBoost: {best_weight:.2f}, DNN: {1 - best_weight:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxD2r-iuWl3a",
        "outputId": "9c446a47-eb39-4b02-8a13-26f6bc599681"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "✅ Best Ensemble RMSE: 40.8437\n",
            "📊 Best Weight → XGBoost: 0.77, DNN: 0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BfnqtSh0qOr",
        "outputId": "ef7756f5-5b69-4ccb-a7af-51edc4c7cfdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
            "🔮 Predicted Delivery Time: 93.22 hours\n",
            "📦 Actual Delivery Time: 80.00 hours\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Ensure X_test and y_test are NumPy arrays\n",
        "X_test_array = np.array(X_test)\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "# Choose a random test index\n",
        "random_index = random.randint(0, len(X_test_array) - 1)\n",
        "\n",
        "# Select single test sample\n",
        "single_order_features = X_test_array[random_index:random_index + 1]\n",
        "actual_delivery_time = y_test_array[random_index]\n",
        "\n",
        "# Predict using both models\n",
        "xgb_pred = xg_reg_best.predict(single_order_features)[0]\n",
        "dnn_pred = dnn_model.predict(single_order_features)[0][0]\n",
        "\n",
        "# Ensemble prediction\n",
        "ensemble_pred = best_weight * xgb_pred + (1 - best_weight) * dnn_pred\n",
        "\n",
        "# Output\n",
        "print(f\"🔮 Predicted Delivery Time: {ensemble_pred:.2f} hours\")\n",
        "print(f\"📦 Actual Delivery Time: {actual_delivery_time:.2f} hours\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
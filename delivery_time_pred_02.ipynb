{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7clLjrQ0rVx",
        "outputId": "7296506e-1ed1-4f34-c2eb-0a2de38e3eb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKFX03CH0qOp",
        "outputId": "c7d9a70b-7c9d-40ab-f7db-db8d13d731ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15-3100086836.py:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_selected['Order_Time'] = pd.to_datetime(df_selected['Order_Time'])\n",
            "/tmp/ipython-input-15-3100086836.py:32: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_selected['Pickup_Time'] = pd.to_datetime(df_selected['Pickup_Time'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
            "Best parameters: {'subsample': 1.0, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 15709.2549 - val_loss: 4255.7441 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 3115.7847 - val_loss: 2030.1581 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 2207.1680 - val_loss: 2027.9120 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 2162.5334 - val_loss: 2004.8718 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2175.6304 - val_loss: 2030.3141 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 2116.8066 - val_loss: 2016.5388 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - loss: 2180.9614 - val_loss: 1927.5863 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 2048.5249 - val_loss: 1876.4137 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 2035.0018 - val_loss: 1829.1871 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 1961.2769 - val_loss: 1823.6322 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 1995.5160 - val_loss: 1823.4591 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 14ms/step - loss: 1959.6742 - val_loss: 1786.8396 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 1923.4716 - val_loss: 1774.4420 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 1909.2458 - val_loss: 1783.2188 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 1900.0045 - val_loss: 1758.1027 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 1903.3160 - val_loss: 1789.7577 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - loss: 1906.7122 - val_loss: 1793.9319 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 1935.5918 - val_loss: 1768.2922 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 1908.2023 - val_loss: 1765.7953 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 1918.7892 - val_loss: 1771.7944 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - loss: 1924.2782 - val_loss: 1750.9478 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1888.1975 - val_loss: 1737.8461 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 1893.4293 - val_loss: 1748.5787 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - loss: 1881.7365 - val_loss: 1740.8048 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 1885.9032 - val_loss: 1738.0082 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 1850.4318 - val_loss: 1743.8743 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 1876.8112 - val_loss: 1727.2947 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 1841.1180 - val_loss: 1741.3269 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 1905.5345 - val_loss: 1729.9740 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 1882.8411 - val_loss: 1730.3015 - learning_rate: 5.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 1868.9397 - val_loss: 1753.0811 - learning_rate: 5.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1866.8986 - val_loss: 1776.7969 - learning_rate: 5.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1835.8374 - val_loss: 1739.8013 - learning_rate: 2.5000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 1828.8474 - val_loss: 1743.3495 - learning_rate: 2.5000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 1839.9836 - val_loss: 1767.1581 - learning_rate: 2.5000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1844.5272 - val_loss: 1729.5001 - learning_rate: 2.5000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m873/873\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 1835.2070 - val_loss: 1752.8717 - learning_rate: 2.5000e-04\n",
            "XGBoost RMSE: 40.91300704553275\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "DNN RMSE: 41.5896513171208\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "file_path = \"amazon_delivery.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "selected_columns = [\n",
        "    'Store_Latitude', 'Store_Longitude', 'Drop_Latitude', 'Drop_Longitude',\n",
        "    'Order_Date', 'Order_Time', 'Pickup_Time', 'Weather', 'Traffic',\n",
        "    'Vehicle', 'Area', 'Delivery_Time'\n",
        "]\n",
        "df_selected = df[selected_columns]\n",
        "\n",
        "df_selected = df_selected.dropna()\n",
        "\n",
        "df_selected['Order_Date'] = pd.to_datetime(df_selected['Order_Date'])\n",
        "df_selected['Day_of_Week'] = df_selected['Order_Date'].dt.dayofweek\n",
        "df_selected['Order_Time'] = pd.to_datetime(df_selected['Order_Time'])\n",
        "df_selected['Pickup_Time'] = pd.to_datetime(df_selected['Pickup_Time'])\n",
        "df_selected['Time_To_Pickup'] = (df_selected['Pickup_Time'] - df_selected['Order_Time']).dt.total_seconds() / 60.0\n",
        "\n",
        "df_selected = df_selected.drop(columns=['Order_Date', 'Order_Time', 'Pickup_Time'])\n",
        "\n",
        "df_selected = pd.get_dummies(df_selected, columns=['Weather', 'Traffic', 'Vehicle', 'Area'])\n",
        "\n",
        "df_selected['Distance'] = np.sqrt(\n",
        "    (df_selected['Drop_Latitude'] - df_selected['Store_Latitude'])**2 +\n",
        "    (df_selected['Drop_Longitude'] - df_selected['Store_Longitude'])**2\n",
        ")\n",
        "\n",
        "X = df_selected.drop(columns=['Delivery_Time'])\n",
        "y = df_selected['Delivery_Time']\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'reg_alpha': [0, 1],\n",
        "    'reg_lambda': [0.1, 1]\n",
        "}\n",
        "\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_grid,\n",
        "                                   n_iter=100, scoring='neg_mean_squared_error',\n",
        "                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "xg_reg_best = xgb.XGBRegressor(**best_params)\n",
        "xg_reg_best.fit(X_train, y_train)\n",
        "\n",
        "def create_dnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "dnn_model = create_dnn_model()\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "history = dnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,\n",
        "                        callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "y_pred_xgboost = xg_reg_best.predict(X_test)\n",
        "rmse_xgboost = np.sqrt(mean_squared_error(y_test, y_pred_xgboost))\n",
        "print(f\"XGBoost RMSE: {rmse_xgboost}\")\n",
        "\n",
        "y_pred_dnn = dnn_model.predict(X_test)\n",
        "rmse_dnn = np.sqrt(mean_squared_error(y_test, y_pred_dnn))\n",
        "print(f\"DNN RMSE: {rmse_dnn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save XGBoost model\n",
        "xg_reg_best.save_model(\"xgboost_model.json\")\n",
        "\n",
        "# Save DNN model\n",
        "dnn_model.save(\"dnn_model.h5\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"xgboost_model.json\")\n",
        "files.download(\"dnn_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "iLPuYjYSUiC7",
        "outputId": "5d2278fa-606f-47b2-86b9-c42455f4e6b2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cad405a7-f1d2-4353-8a66-cc7adca70515\", \"xgboost_model.json\", 232966)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85f901dc-5c44-4ae0-9211-7ad466a1221d\", \"dnn_model.h5\", 4031536)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Predict using both trained models\n",
        "y_pred_xgb = xg_reg_best.predict(X_test)\n",
        "y_pred_dnn = dnn_model.predict(X_test).flatten()\n",
        "\n",
        "# Search for best weight to combine them\n",
        "best_rmse = float(\"inf\")\n",
        "best_weight = 0.0\n",
        "\n",
        "weights = np.linspace(0, 1, 101)  # Try weights from 0.00 to 1.00 (step of 0.01)\n",
        "\n",
        "for w in weights:\n",
        "    y_pred_combined = w * y_pred_xgb + (1 - w) * y_pred_dnn\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_combined))\n",
        "\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse = rmse\n",
        "        best_weight = w\n",
        "\n",
        "# Final ensemble prediction using the best weight\n",
        "y_pred_ensemble = best_weight * y_pred_xgb + (1 - best_weight) * y_pred_dnn\n",
        "\n",
        "print(f\"âœ… Best Ensemble RMSE: {best_rmse:.4f}\")\n",
        "print(f\"ğŸ“Š Best Weight â†’ XGBoost: {best_weight:.2f}, DNN: {1 - best_weight:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxD2r-iuWl3a",
        "outputId": "9c446a47-eb39-4b02-8a13-26f6bc599681"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "âœ… Best Ensemble RMSE: 40.8437\n",
            "ğŸ“Š Best Weight â†’ XGBoost: 0.77, DNN: 0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BfnqtSh0qOr",
        "outputId": "ef7756f5-5b69-4ccb-a7af-51edc4c7cfdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
            "ğŸ”® Predicted Delivery Time: 93.22 hours\n",
            "ğŸ“¦ Actual Delivery Time: 80.00 hours\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Ensure X_test and y_test are NumPy arrays\n",
        "X_test_array = np.array(X_test)\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "# Choose a random test index\n",
        "random_index = random.randint(0, len(X_test_array) - 1)\n",
        "\n",
        "# Select single test sample\n",
        "single_order_features = X_test_array[random_index:random_index + 1]\n",
        "actual_delivery_time = y_test_array[random_index]\n",
        "\n",
        "# Predict using both models\n",
        "xgb_pred = xg_reg_best.predict(single_order_features)[0]\n",
        "dnn_pred = dnn_model.predict(single_order_features)[0][0]\n",
        "\n",
        "# Ensemble prediction\n",
        "ensemble_pred = best_weight * xgb_pred + (1 - best_weight) * dnn_pred\n",
        "\n",
        "# Output\n",
        "print(f\"ğŸ”® Predicted Delivery Time: {ensemble_pred:.2f} hours\")\n",
        "print(f\"ğŸ“¦ Actual Delivery Time: {actual_delivery_time:.2f} hours\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}